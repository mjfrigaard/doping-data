Part 01.0: United States Anti-Doping Agency (USADA) Sanction Data -
Downloading data with `xml` and `rvest`
================
Martin Frigaard
2020-06-08

# Motivation

I wanted to know if there were any relationships between athletes and
the substances they get caught using. So this document creates a dataset
of US anti-doping agency (USADA) data for athletes and the sports they
participate in.

## The Data Source

These data comes from the Sanctions table on the USADA
[website.](https://www.usada.org/testing/results/sanctions/)

<img src="figs/usada-sanctions.png" width="2862" />

-----

In order to extract data from the website, I’ll be using the [`rvest`
package](https://cran.r-project.org/web/packages/rvest/index.html)
written by Hadley Wickham to scrape the `html` code. The code chunk
below loads the packages needed to reproduce the graphics.

``` r
library(tidyverse)
library(rvest)
library(methods)
library(magrittr)
library(ggthemes)
library(extrafont)
library(ggplot2)
library(gridExtra)
library(wesanderson)
library(tidytext)
```

-----

## Scraping the USADA website

The website for these data is available
[here](https://tinyurl.com/yc346fq5). The `rvest::read_html()` and
`rvest::html_nodes()` functions extract the content from the table in
the Web page and translates it from HTML into a data frame.

``` r
USADA_url <- "https://www.usada.org/testing/results/sanctions/"
USADA_extraction <- USADA_url %>%
     read_html() %>%
     html_nodes("table")
```

> **Store and explore**\* refers to storing an output of a call to an
> object, then checking the contents of that new object. This is a great
> way to get to know R, objet-oriented programming, and how functions
> work together in packages.

### Check the structure of the extraction

Look at the structure of `USADA_extraction`.

``` r
# check the structure of the new USADA_extraction object
USADA_extraction %>% str()
```

    #>  List of 1
    #>   $ :List of 2
    #>    ..$ node:<externalptr> 
    #>    ..$ doc :<externalptr> 
    #>    ..- attr(*, "class")= chr "xml_node"
    #>   - attr(*, "class")= chr "xml_nodeset"

This contains a `node` and a `doc` in the List of 2.

### Check the class of the extraction

If I check the class of the list we extracted, we find…

``` r
USADA_extraction %>% class()
```

    #>  [1] "xml_nodeset"

…this is an `xml_nodeset` with 2 lists (stored within the 1 list). The
data we want from this object is in position `[[1]]` of this list. I can
subset the `USADA_extraction` list with the `rvest::html_table()`
function and store the table contents in the `UsadaRaw` object. I check
my work using the `dplyr::glimpse(70)`.

> why `dplyr::glimpse(70)`? It prints less to the screen and keeps the
> col width to \<80, which is nice for working in plain text.

``` r
UsadaRaw <- rvest::html_table(USADA_extraction[[1]])
UsadaRaw %>% dplyr::glimpse(70)
```

    #>  Rows: 763
    #>  Columns: 5
    #>  $ Athlete              <chr> "Walsh, Cole", "Romero Noboa, Isidro",…
    #>  $ Sport                <chr> "Track and Field", "Triathlon", "Mixed…
    #>  $ `Substance/Reason`   <chr> "Cannabinoids", "Androgenic Anabolic S…
    #>  $ `Sanction Terms`     <chr> "6-Month Suspension with 3-Month Defer…
    #>  $ `Sanction Announced` <chr> "06/01/2020", "05/22/2020", "05/11/202…

This reveals a data frame with 763 observations. The contents from the
HTML list (`USADA_extraction`) has been converted to a data frame
(`UsadaRaw`). I’m going to store this data frame as a .csv in a
`data/raw` folder (so I don’t have to scrape it every time I run this
script).

``` r
# create data path
raw_data_path <- paste0("data/raw/", base::noquote(lubridate::today()))
raw_data_path
```

    #>  [1] "data/raw/2020-06-08"

``` r
# create a new data folder
if (!file.exists(raw_data_path)) {
     dir.create(raw_data_path)
}
fs::dir_tree("data/raw", recurse = FALSE)
```

    #>  data/raw
    #>  ├── 2020-02-25
    #>  ├── 2020-03-21
    #>  └── 2020-06-08

Great\! I can export these data into the new folder I just created. Now
that I have a time-stamped data set, I will export it as a .csv file.

``` r
# export the .csv file
write_csv(as.data.frame(UsadaRaw), 
          paste0(raw_data_path, "/",
                           "UsadaRaw-",
                           base::noquote(lubridate::today()),
                           ".csv"))
# export as a .RData file, too.
save.image(file = paste0(raw_data_path, "/",
                         "UsadaRaw-", 
                         base::noquote(lubridate::today()),
                         ".RData"))
# check
fs::dir_tree("data/raw")
```

    #>  data/raw
    #>  ├── 2020-02-25
    #>  │   ├── UsadaRaw-2020-02-25.RData
    #>  │   └── UsadaRaw-2020-02-25.csv
    #>  ├── 2020-03-21
    #>  │   ├── UsadaRaw-2020-03-21.RData
    #>  │   └── UsadaRaw-2020-03-21.csv
    #>  └── 2020-06-08
    #>      ├── UsadaRaw-2020-06-08.RData
    #>      └── UsadaRaw-2020-06-08.csv

So we can see I’ve been scraping these data for awhile now (and storing
them in dated folders). We will start wrangling the doping data for
visualizations in the next posting.
