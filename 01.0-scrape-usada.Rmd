---
title: 'Part 01.0: United States Anti-Doping Agency (USADA) Sanction Data - Downloading data with `xml` and `rvest`'
author: "Martin Frigaard"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
# create figs folder ----
if (!file.exists("figs/")) {
    dir.create("figs/")
}
# create data folder ----
if (!file.exists("data/")) {
    dir.create("data/")
}
# create docs folder ----
if (!file.exists("docs/")) {
    dir.create("docs/")
}
library(knitr)
library(rmdformats)
library(tidyverse)
library(devtools)
library(hrbrthemes)
# chunks set options
knitr::opts_chunk$set(
  echo = TRUE, # show/hide all code
  # results = "hide", # hide results
  tidy = FALSE, # cleaner code printing
  comment = "#> ", # better console printing
  eval = TRUE, # turn this to FALSE stop code chunks from running
  message = TRUE, # show messages
  warning = FALSE, # show warnings
  size = "small", # size of the text
  fig.path = "figs/", # location of files
  fig.height = 5.5, # height of figures
  fig.width = 8 # width of figures
) # width of figures
# knit set options
knitr::opts_knit$set(
  width = 78,
  progress = FALSE
)
# base options
base::options(
  tibble.print_max = 25,
  tibble.width = 78,
   max.print = 999999,
  scipen = 100000000
)
```

# Motivation

I wanted to know if there were any relationships between athletes and the substances they get caught using. So this document creates a dataset of US anti-doping agency (USADA) data for athletes and the sports they participate in. 

## The Data Source

These data comes from the Sanctions table on the USADA [website.](https://www.usada.org/testing/results/sanctions/) 

```{r usada-sanctions.png, echo=FALSE}
knitr::include_graphics("figs/usada-sanctions.png")
```


***

In order to extract data from the website, I'll be using the [`rvest` package](https://cran.r-project.org/web/packages/rvest/index.html) written by Hadley Wickham to scrape the `html` code. The code chunk below loads the packages needed to reproduce the graphics. 

```{r packages_download_data, message=FALSE, warning=FALSE, eval=TRUE}
library(tidyverse)
library(rvest)
library(methods)
library(magrittr)
library(ggthemes)
library(extrafont)
library(ggplot2)
library(gridExtra)
library(wesanderson)
library(tidytext)
```

***

## Scraping the USADA website

The website for these data is available [here](https://tinyurl.com/yc346fq5). The `rvest::read_html()` and `rvest::html_nodes()` functions extract the content from the table in the Web page and translates it from HTML into a data frame.

```{r read_html_USADA, eval=TRUE, message=FALSE, warning=FALSE}
USADA_url <- "https://www.usada.org/testing/results/sanctions/"
USADA_extraction <- USADA_url %>%
     read_html() %>%
     html_nodes("table")
```



> **Store and explore*** refers to storing an output of a call to an object, then checking the contents of that new object. This is a great way to get to know R, objet-oriented programming, and how functions work together in packages. 

### Check the structure of the extraction

Look at the structure of `USADA_extraction`.

```{r USADA_extraction_str, eval=TRUE}
# check the structure of the new USADA_extraction object
USADA_extraction %>% str()
```

This contains a `node` and a `doc` in the List of 2. 

### Check the class of the extraction

If I check the class of the list we extracted, we find...

```{r USADA_extraction_class, eval=TRUE}
USADA_extraction %>% class()
```

...this is an `xml_nodeset` with 2 lists (stored within the 1 list). The data we want from this object is in position `[[1]]` of this list. I can subset the `USADA_extraction` list with the `rvest::html_table()` function and store the table contents in the `UsadaRaw` object. I check my work using the `dplyr::glimpse(70)`.

> why `dplyr::glimpse(70)`? It prints less to the screen and keeps the col width to <80, which is nice for working in plain text.

```{r UsadaRaw, eval=TRUE}
UsadaRaw <- rvest::html_table(USADA_extraction[[1]])
UsadaRaw %>% dplyr::glimpse(70)
```


This reveals a data frame with `r nrow(UsadaRaw)` observations. The contents from the HTML list (`USADA_extraction`) has been converted to a data frame (`UsadaRaw`). I'm going to store this data frame as a .csv in a `data/raw` folder (so I don't have to scrape it every time I run this script). 
```{r raw_data_path, eval=TRUE}
# create data path
raw_data_path <- paste0("data/raw/", base::noquote(lubridate::today()))
raw_data_path
# create a new data folder
if (!file.exists(raw_data_path)) {
     dir.create(raw_data_path)
}
fs::dir_tree("data/raw", recurse = FALSE)
```

Great! I can export these data into the new folder I just created. Now that I have a time-stamped data set, I will export it as a .csv file. 

```{r export_raw, eval=TRUE}
# export the .csv file
write_csv(as.data.frame(UsadaRaw), 
          paste0(raw_data_path, "/",
                           "UsadaRaw-",
                           base::noquote(lubridate::today()),
                           ".csv"))
# export as a .RData file, too.
save.image(file = paste0(raw_data_path, "/",
                         "UsadaRaw-", 
                         base::noquote(lubridate::today()),
                         ".RData"))
# check
fs::dir_tree("data/raw")
```

So we can see I've been scraping these data for awhile now (and storing them in dated folders). We will start wrangling the doping data for visualizations in the next posting. 